{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ffd0d7-951a-4d14-82ea-720894f75db4",
   "metadata": {},
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aabd6f-f57f-4dd6-89e8-a9086eb153c6",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Требуется обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Необходимо построить модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e0d9b-ab8a-49f2-a9fa-1a5d7461b1f5",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7d2a017c-78c4-455a-a15d-99171b90e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d776f531-9cb2-4afd-b17f-318f35a7b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('toxic_comments.csv', index_col=0)\n",
    "except:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d690dc24-a6b4-4ce2-bd8c-b745d786ecb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a9aa66-d270-45fa-8fb0-3ea7b6640d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2cc1c-f801-4171-8668-2ab656043790",
   "metadata": {},
   "source": [
    "Пропусков и явных дубликатов нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e1f793-a2b2-44d5-8177-a45ad4cfebc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13235961-76ce-4400-90d9-2f677f125e58",
   "metadata": {},
   "source": [
    "Отмечаем, что язык комментариев - английский."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2415516f-aeb4-4874-b641-c3ee42e8afef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "948e1e69-b48c-496c-955f-10ac33b4bc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.841344371679229"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = df['toxic'].value_counts()[0] / df['toxic'].value_counts()[1]\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedae744-d196-4175-806c-8dd22cd1b237",
   "metadata": {},
   "source": [
    "Сильный перекос в сторону одного из значений целевого признака.  \n",
    "Стоит учесть это при обучении моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ff95e-8343-46e6-837d-a386ccfc6a3a",
   "metadata": {},
   "source": [
    "Для лемматизации будем использовать Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee25719-2add-4c2f-9f4d-29aa7dd8538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm', disable=['parser', 'nem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9e63496-6068-4cce-bdef-d51a52662b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    sp = spacy.load('en_core_web_sm', disable=['parser', 'nem'])\n",
    "    doc = sp(text)\n",
    "    lemm = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc95513-cb5d-4686-840a-e3d78d24bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    a = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    b = a.split()\n",
    "    c = \" \".join(b)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb2200-db4f-40a8-b701-8964d30a5023",
   "metadata": {},
   "source": [
    "Лемматизация всего объема данных, даже после даунсемплинга, может занять около суток. Поскольку мы все-таки ограничены во времени, далее будем работать с выборкой в 10000 комментариев, на локальном устройстве лемматизация заняла около полутора часов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170e5e8d-4000-42bd-832b-5e5f706a5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(n=10000, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54672745-2069-4106-87ba-92ffb5873a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.8982\n",
       "1    0.1018\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e777f127-caa1-4883-9767-5aa765bf2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sample_df['text']\n",
    "target = sample_df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b84a5cd0-3e0d-49a4-9c02-59345d014d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 31min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_lemm = features.apply(lambda x: lemmatize(clear_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "146b880a-3083-4d31-baaf-ebdfbd767743",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lemm_train, features_lemm_test, target_train, target_test = train_test_split(features_lemm, target, test_size=0.2, random_state=999)\n",
    "features_lemm_train, features_lemm_valid, target_train, target_valid = train_test_split(features_lemm_train, target_train, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a1519-dc54-443a-ba3f-405559db8bbf",
   "metadata": {},
   "source": [
    "Загружаем словарь стоп-слов для английского языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bfe1511-e882-40af-83cd-cc6d1986ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Breaktroo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3594e3-d362-4fc0-a2be-0758366a93b5",
   "metadata": {},
   "source": [
    "Создадим, обучим и применим счетчик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "dd789876-ac33-4a56-a458-af1defff39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(binary=True, stop_words=stopwords, max_features=20600)\n",
    "tf_idf_train = count_tf_idf.fit_transform(features_lemm_train.values)\n",
    "tf_idf_valid = count_tf_idf.transform(features_lemm_valid.values)\n",
    "tf_idf_test = count_tf_idf.transform(features_lemm_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5c65103d-4554-44ef-8add-43296449932c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 21000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2000, 21000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2000, 21000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tf_idf_train.shape)\n",
    "display(tf_idf_valid.shape)\n",
    "display(tf_idf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102b6aa-114f-42d3-89bd-a99de56fa8c1",
   "metadata": {},
   "source": [
    "Признаки готовы.  \n",
    "Переходим к обучению моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce870c6-23ac-49b0-bab2-9596b7b6c9b5",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf2e4e-2197-402a-b452-f3e115620ca0",
   "metadata": {},
   "source": [
    "Начнем с модели случайного леса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "ae02c5bf-ff5b-42a7-ae96-d73e7499a9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best result'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.4492131616595135"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'best depth'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'best leaves'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_depth = 0\n",
    "best_leaves = 0\n",
    "for leaves in range(1, 10, 2):\n",
    "    for depth in range(1, 10):\n",
    "        model = RandomForestClassifier(max_depth=depth, min_samples_leaf=leaves, class_weight='balanced', n_estimators=100, random_state=12345)\n",
    "        model.fit(tf_idf_train, target_train)\n",
    "        prediction = model.predict(tf_idf_valid)\n",
    "        result = f1_score(target_valid, prediction)\n",
    "        if result > best_result:\n",
    "            best_model = model\n",
    "            best_result = result\n",
    "            best_depth = depth\n",
    "            best_leaves = leaves\n",
    "display('best result', best_result)\n",
    "display('best depth', best_depth)\n",
    "display('best leaves', best_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b0e7f-6d92-4521-90c0-f5bbb06dc80d",
   "metadata": {},
   "source": [
    "Ужасный результат.  \n",
    "Далее проверим логистическую регрессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d6936635-714a-4a82-b91f-a5695cfd5060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 672 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7500000000000001"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(penalty='none', class_weight='balanced', n_jobs=-1, max_iter=1000, random_state=12345)\n",
    "model.fit(tf_idf_train, target_train)\n",
    "prediction = model.predict(tf_idf_valid)\n",
    "result = f1_score(target_valid, prediction)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c2ad34-6394-473c-a3b9-d08b0f80e051",
   "metadata": {},
   "source": [
    "Нужное значение получено.  \n",
    "Теперь рассмотрим градиентный бустинг LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "526ae45c-085e-45a7-8240-25684f576de3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best result'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6165048543689321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'best depth'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'best leaves'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_depth = 0\n",
    "best_leaves = 0\n",
    "for leaves in range(6, 51, 5):\n",
    "    for depth in range(1, 10):\n",
    "        model = LGBMClassifier(max_depth=depth, num_leaves=leaves, objective='binary', class_weight='balanced', n_estimators=150, random_state=12345)\n",
    "        model.fit(tf_idf_train, target_train)\n",
    "        prediction = model.predict(tf_idf_valid)\n",
    "        result = f1_score(target_valid, prediction)\n",
    "        if result > best_result:\n",
    "            best_model = model\n",
    "            best_result = result\n",
    "            best_depth = depth\n",
    "            best_leaves = leaves\n",
    "display('best result', best_result)\n",
    "display('best depth', best_depth)\n",
    "display('best leaves', best_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3eb8a7-b068-4fc6-bd1b-efecfffcc8e6",
   "metadata": {},
   "source": [
    "Увы, лучший результат не дотягивает до целевого показателя.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1081a4-a96a-4c5b-8e79-857785b02533",
   "metadata": {},
   "source": [
    "Единственной моделью показавшей нужный результат на валидации стала логистическая регрессии, проверяем её на тестовой выборке.\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "7661224a-6e0b-4228-b89c-63e5887c64a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 708 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7560321715817694"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(penalty='none', class_weight='balanced', n_jobs=-1, max_iter=1000, random_state=12345)\n",
    "model.fit(tf_idf_train, target_train)\n",
    "prediction = model.predict(tf_idf_test)\n",
    "f1_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ab942-4503-43f4-b64c-249ee46806be",
   "metadata": {},
   "source": [
    "Необходимое значение F1 получено!  Осталось сравнить результат с показателями дамми-модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "fbfb389d-7e02-40e1-9760-278f43175a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18594104308390022"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "dummy_clf.fit(tf_idf_train, target_train)\n",
    "predictions_dummy = dummy_clf.predict(tf_idf_test)\n",
    "dummy_clf_result = f1_score(target_test, predictions_dummy)\n",
    "dummy_clf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cec5a3-a3a3-4887-af8d-c04805bb62bc",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e334b-e01c-4a7b-9403-5ddc35247801",
   "metadata": {},
   "source": [
    "Предоставленные данные в целом пригодны для выполнения задачи.  \n",
    "Однако тот факт, что текст не лемматизирован, требует либо несоизмеримых выислительных мощностей, либо большого количества времени на лемматизацию.  \n",
    "В целях сокращения времени выполнения задания, было принято решение использовать выборку из датасета размером в 1000 комментариев.  \n",
    "Данная выборка была лемматизирована при помощи Spacy, с помощью алгоритма TF-IDF были созданы признаки для обучения модели.  \n",
    "Из нескольких обученных моделей, с учетом размера исходного датасета, была выбрана логистическая регрессия, показавшая необходимый результат на тестовой выборке и прошла проверку адекватности при сравнении с дамми-моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714427e7-cc8a-4d83-9a0d-5364cdb78b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=1234)] + [features_ones])\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=1234)] + [target_ones])\n",
    "    \n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=1234)\n",
    "    \n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "features, target = downsample(df['text'], df['toxic'], 0.12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
